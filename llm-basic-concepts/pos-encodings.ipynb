{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cfdb8ab",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26846989",
   "metadata": {},
   "source": [
    "1. Positional encoding required to make sure model understands position of a word in the given sentence.\n",
    "\n",
    "2. Same word can be different positions and have different meanings. If we rely only on token embeddings a word will have same information irrespective of its position in a sentence which will reduce model's performance.\n",
    "\n",
    "3. We somehow need to embed this positional information into input of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d1251",
   "metadata": {},
   "source": [
    "### Types\n",
    "\n",
    "1. **Absolute positional embedding**:\n",
    "\n",
    "    A distinct value for each position in the sequence. More details https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n",
    "\n",
    "\n",
    "    It can use either learnable embedding layer or  sine and cosine functions to generate distinct vector for each position of a vector.\n",
    "\n",
    "\n",
    "    token embeddings + positional embedding = input vector to the model which is then used for attention scoring (q.T dot k)\n",
    "\n",
    "\n",
    "2. **Relative positional embedding**:\n",
    "\n",
    "    A separate set of vectors representing the relative position is created (learnable or fixed).\n",
    "\n",
    "    E.g. if context size is 4, relative positions can be from -3 to +3.\n",
    "\n",
    "    r = {\n",
    "        -3 : vector,\n",
    "        -2 : different vector\n",
    "        -1 : another vector\n",
    "        ...\n",
    "        +3 : some other vector\n",
    "    }\n",
    "\n",
    "\n",
    "    Attention score = q.T dot k + q.T dot r"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
